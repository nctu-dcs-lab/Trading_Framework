## Description
I'm trying to create an automated index trader that trades on MTX and makes a steady profit. This repository mainly references the paper **Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy** and applies its methods to MTX.

## Summary
In Experiments 1 to 12, only Experiments 11 and 12 are noteworthy; the other experiments were just for preliminary testing. In Experiment 11, I trained two agents: one could take both long and short positions, while the other could only take long positions. In Experiment 12, the agent could also take both long and short positions, but its reward was adjusted by adding the Sharpe ratio to encourage the agent to consider risk when trading. The results are shown below.
| experiment | ROI | Sharp ratio |
| ---- | ---- | ---- |
|11-1 (Long)| 823% | 0.29 |
|11-2 (Long/Short)| 843% | 0.3 |
|12 (Long/Short)| 713% | 0.27 |

> **Info** 
> The environment in Experiments 1 to 12 does not consider margin or invalid actions, so the results of those experiments are only for reference and do not hold any value.

It seems that adding the Sharpe ratio to the reward function didn't have an effect. Additionally, these three experiments have some issues: they don't account for the margin, and they all use leverage, which makes the ROI excessively high.

In Experiments 13 to 18, I used action masking to prevent the agent from outputting invalid actions (a problem observed in Experiments 1 to 12) and to allow the agent to observe more information at once. The goal was to make its performance comparable to the non-action-masking version. However, the results showed that it performed worse with action masking. I noticed that decreasing the initial balance improved performance, so I limited the agent to trading only one contract at a time, and the results looked good. I also tried adding a stop-loss strategy, and it seems that adding a stop-loss improves performance. The results are shown below.

initial balance : 100000
| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|18 (no stoploss)| 100% | 26% | 0.19 |
|18 (stoploss 50 point) | 327% | 62% | 0.73 |
|18 (stoploss 75 point) | 204% | 44% | 0.43 |
|18 (stoploss 100 point) | 235% | 49% | 0.42 |

In Experiment 18, I still did not consider margin. Next, I will try to incorporate the margin system and ensure that the agent can still profit when trading a larger number of contracts.

> **Info** 
> In Experiments 13 to 18, the environment still does not consider margin and can only trade one share at a time, so the results are only for reference.


## Experiment 01
Experiment 01 was trained from 2018 to 2022 and validated in 2023. The environment only allows taking long positions, with an initial balance of 1 million. The results show that the mean reward is 33.93 and the standard deviation is 61.73. It seems our agent performed well in 2023, but there are some issues. The agent takes some unexpected actions, such as buying at the beginning and selling on the last day. This behavior appears unusual, but I think it is because 2023 was a bull market, which means this kind of strategy could be quite profitable.

## Experiment 02
In Experiment 02, I changed the balance to 10 million to check if the actions generated by the agent were effective. Unexpected actions still appeared in 2023. It seems my opinion is correct that 2023 was a bull market, so the strategy the agent is using makes sense.

## Experiment 03
In Experiment 03, I set different configurations by changing the batch size to 32 and the number of steps to 2048, and the total timesteps to 5 million. I also added some measures to evaluate model performance, such as RoR, Sharpe ratio, and MDD. The results show that the mean reward is -0.98 with a standard deviation of 6.43. It seems this agent is not performing better than the Experiment 01 agent.

## Experiment 04
In Experiment 04, I plan to try various configurations to find the best one suitable for this auto-trade task.


## Experiment 05
In Experiment 05, I changed the training range, and the results show that the mean reward is 17.35, the standard deviation is 83.22, the mean Sharpe ratio is 0.69, and the mean MDD is -66.2. However, it still takes some unexpected actions, such as buying the index at the start of the month and selling it at the end of the month.

## Experiment 06
In Experiment 06, I allowed the agent to take both long and short positions, but the results are the same as in Experiment 04-2. The agent still takes unexpected actions, and I can't figure out why it doesn't take short positions in Experiment 06.

## Experiment 07
In Experiment 07, I used a learning rate scheduler and increased the total timesteps to 5 million. The results show that the mean reward is 32.56, with a standard deviation of 61.

## Experiment 08
In Experiment 08, I changed the initial balance to 100,000 and set the learning rate schedule. The results seem to have an issue, so I'll check them again. After that, I'll finish the report.

## Experiment 09
In Experiment 09, I set the total timesteps to 1 billion, the learning rate to 1e-6, and the initial balance to 100,000. I also used a learning rate scheduler. The results seem to have an issue, so I'll check them again. After that, I'll finish the report.

## Experiment 10
In Experiment 10, I set the total timesteps to 1 billion, the learning rate to 1e-6, and the initial balance to 1 million. I also used a learning rate scheduler. The result show that the mean reward is 32.56 and std is 61.31.

## Experiment 11
In Experiment 11, I compare two different agents: one agent is allowed to take both long and short positions, while the other agent is allowed to take only long positions.

## Observations and Conclusions of Experiment 1 to 11
In some experiments, like Experiment 06, 07, and 10, they achieved the same performance in the validation set. I think this is because I set the deterministic parameter to True, leading to the same actions in the validation set. I also observed that the agent struggles to learn how to take short positions, likely due to market conditions from 2018 to 2022 being more favorable for long positions, with only a few situations suitable for short positions.

I think I could train two agents: one for long positions and one for short positions, and then combine them to handle both. In the current experiment, the agent outputs the number of shares it wants to trade, but I want to change this to a confidence score. For example, a confidence score of 0.9 would indicate that the agent strongly believes buying is a good choice and will buy more, while a score of 0.5 would mean the agent sees it as a decent but not strong choice. A negative confidence score would indicate that the agent wants to sell the index.

## Experiment 13
In Experiment 13, I'm trying to train two agents: one that can only trade in long positions and another that can only trade in short positions. The results show that the short-position agent does not perform well.

## Experiment 14
In Experiment 14, I'm using action masking to prevent the agent from choosing invalid actions. I hoped this mechanism would improve performance, but the results show that its performance is worse than when not using action masking.

## Experiment 15 & 16
In Experiments 15 and 16, I'm using action masking to prevent the agent from choosing invalid actions. I also let the agent see more information than just one time step, but the results show that it's still performing worse than without action masking.


## Experiment 17
In Experiment 17, I changed the reward function. The reward function in the environment "StockMarketLongDiscreteMaskV2" only considers the agent's position at that moment. I trained two agents in this experiment: one agent has a starting balance of 10k, while the other has a starting balance of 1M. The results show that the first agent achieves a mean reward of 25%, while the second agent has a mean reward of -1.25%.

## Experiment 18
In Experiment 18, I restricted the action space to between -1 and 1 to train an agent to trade at an optimal point. If this works, I plan to further train the agent to consider balance. I tested the agent in two scenarios: one with no stop loss and one with stop loss. The results show that with stop loss, the agent's performance is higher, with an ROI of 327%, an IRR of 62%, and a Sharpe ratio of 0.73.

- initial balance : 100000
- train env : StockEnvOnlyOneShare-v1
- test env : StockEnvOnlyOneShareTesting-v1
- test date : 2021~2023

| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|18 (no stoploss)       | 100%  | 26% | 0.19 |
|18 (stoploss 25 point) | 291%  | 57% | 0.64 |
|18 (stoploss 50 point) | 327%  | 62% | 0.73 |
|18 (stoploss 75 point) | 204%  | 44% | 0.43 |
|18 (stoploss 100 point)| 235%  | 49% | 0.42 |
|18 (stoploss 125 point)| 126%  | 31% | 0.26 |
|18 (stoploss 150 point)| 80%   | 21% | 0.16 |
|18 (stoploss 175 point)| 179%  | 40% | 0.35 |
|18 (stoploss 200 point)| 100%  | 26% | 0.22 |

## Observations and Conclusions of Experiment 13 to 18
As the results of Experiment 13 show, `the agent couldn't learn how to take short positions`, so I changed my focus to long positions only. Once the agent can handle long positions properly, I will try short positions again. In Experiment 14, I applied action masking to the training process to mask invalid actions, so the agent would not output invalid actions. However, it performed worse when action masking was applied with a higher initial balance (larger action space). To address this, I trained an agent that could only trade one contract at a time (Experiment 18 with a smaller action space), and it became more like a trading point predictor rather than an auto trader. It performed well when I limited the number of trades.

## Experiment 19 
In Experiment 19, I retrained the model from Experiment 18, allowing the agent to trade more than one contract at a time and setting the initial balance to 100,000. The results show that without a stop-loss strategy, it achieved a Sharpe ratio of 0.39, ROI of 308%, and IRR of 59%. With a stop-loss strategy, it reached a Sharpe ratio of 1.5, ROI of 210%, and IRR of 45%.

## Experiment 20
In Experiment 20, I added a margin system to the environment to make it more realistic. The results show that it achieved a Sharpe ratio of 0.37, ROI of 192%, and IRR of 43% without a stop-loss strategy. With a stop-loss strategy, when the stop-loss is set to 25, it achieved a Sharpe ratio of 0.57, ROI of 240%, and IRR of 50%. For other stop-loss points, please refer to the table below.

- initial balance : 100000
- train env : StockEnvNoLimit-v1
- test env : StockEnvMargin-v1
- test date : 2021~2023
  
| experiment             | ROI   |IRR      | Sharp ratio |
| ----                   | ----  | ----    | ----        |
|20 (no stoploss)        | 192% | 43%     | 0.18        |
|20 (stoploss 25 point)  | 240%  | 50%     | 0.57        |
|20 (stoploss 50 point)  | 75%   | 20%     | 0.23        |
|20 (stoploss 75 point)  | -14%  | -0.04%  | -0.07       |
|20 (stoploss 100 point) | 148%  | 35%     | 0.32        |
|20 (stoploss 125 point) | 19%   | 6%      | 0.03        |
|20 (stoploss 150 point) | 8%    | 2%      | 0.008       |
|20 (stoploss 175 point) | -5%   | -1.7%   | -0.02       |
|20 (stoploss 175 point) | 8%    | 2.6%    | 0.006       |


The impact of adding a stop-loss strategy on performance is not significant. The performance is generally poorer based on the size of the stop-loss setting.


## Experiment 21
In Experiment 21, the hyperparameters are essentially the same as in Experiment 20, with the only difference being that the initial balance is increased to 500,000. The results show that without a stop-loss strategy, the model achieved an ROI of 638%, an IRR of 94%, and a Sharpe ratio of 0.40. When using a stop-loss strategy set to 50 points, the model achieved an ROI of 337%, an IRR of 63%, and a Sharpe ratio of 0.65. For results with other stop-loss points, please refer to the table below.

Based on these results, we can observe that while the stop-loss strategy reduces profitability, it also lowers risk, as indicated by the higher Sharpe ratio. However, the improvement in risk-adjusted returns is not significant enough to justify the sharp decrease in profit. In my opinion, adding a stop-loss strategy does not provide sufficient benefits, as it leads to almost a 50% reduction in profit.


- initial balance : 500000
- train env : StockEnvNoLimit-v1
- test env : StockEnvMargin-v2
- test date : 2021~2023
  
| experiment             | ROI   |IRR      | Sharp ratio |
| ----                   | ----  | ----    | ----        |
|21 (no stoploss)        | 638%  | 94%     | 0.40        |
|21 (stoploss 25 point)  | 295%  | 57%     | 0.66        |
|21 (stoploss 50 point)  | 337%  | 63%     | 0.65        |
|21 (stoploss 75 point)  | 199%  | 44%     | 0.53        |
|21 (stoploss 100 point) | 391%  | 70%     | 0.50        |
|21 (stoploss 125 point) | 97%   | 25%     | 0.24        |
|21 (stoploss 150 point) | 178%  | 40%     | 0.27        |
|21 (stoploss 175 point) | 214%  | 46%     | 0.31        |
|21 (stoploss 200 point) | 51%   | 14%     | 0.08        |

## Experiment 22
In Experiment 22, the hyperparameters are essentially the same as those in Experiment 21, with the only difference being the use of the model from Experiment 19 as the pre-trained model. The results show that without a stop-loss strategy, the model achieved an ROI of 774%, an IRR of 106%, and a Sharpe ratio of 0.43. When the stop-loss was set to 25 points, the model achieved an ROI of 292%, an IRR of 57%, and a Sharpe ratio of 0.64. For results with other stop-loss points, please refer to the table below.

The results are similar to those of Experiment 21: while using a stop-loss strategy reduces profits and mitigates risk, the reduction in risk is not substantial enough to justify the significant loss in profitability.

- initial balance : 500000
- train env : StockEnvNoLimit-v1
- test env : StockEnvMargin-v2
- test date : 2021~2023
  
| experiment             | ROI   |IRR      | Sharp ratio |
| ----                   | ----  | ----    | ----        |
|22 (no stoploss)        | 774%  | 106%    | 0.43        |
|22 (stoploss 25 point)  | 292%  | 57%     | 0.64        |
|22 (stoploss 50 point)  | 335%  | 63%     | 0.64        |
|22 (stoploss 75 point)  | 240%  | 50%     | 0.56        |
|22 (stoploss 100 point) | 402%  | 71%     | 0.61        |
|22 (stoploss 125 point) | 336%  | 63%     | 0.55        |
|22 (stoploss 150 point) | 283%  | 56%     | 0.48        |
|22 (stoploss 175 point) | 415%  | 72%     | 0.48        |
|22 (stoploss 200 point) | 361%  | 66%     | 0.40        |

## Experiment 23
In Experiment 23, the setup is similar to that of Experiment 21. However, I retrained the model from Experiment 18 using a different environment. In this new environment, the contract price dynamically changes, making it more realistic.

The result show that without the stop-loss strategy, the model achieved 167% of ROI, 38% of IRR and 0.14 of sharp ratio. With stoploss set to 25 it achieved 324% ROI, 61% of IRR and 0.62 of sharp ratio. For results with other stop-loss points, please refer to the table below.

- initial balance : 500000
- train env : StockEnvNoLimit-v2
- test env : StockEnvMargin-v2
- test date : 2021~2023
  
| experiment             | ROI   |IRR      | Sharp ratio |
| ----                   | ----  | ----    | ----        |
|23 (no stoploss)        | 167%  | 38%     | 0.14        |
|23 (stoploss 25 point)  | 324%  | 61%     | 0.62        |
|23 (stoploss 50 point)  | 306%  | 59%     | 0.51        |
|23 (stoploss 75 point)  | 51%   | 14%     | 0.11        |
|23 (stoploss 100 point) | 296%  | 58%     | 0.48        |
|23 (stoploss 125 point) | -1%   | -0.4%   | -0.01       |
|23 (stoploss 150 point) |  1%   | 0.3%    | -0.01       |
|23 (stoploss 175 point) | 57%   | 16%     | 0.09        |
|23 (stoploss 200 point) | 4.5%  | 1.4%    | -0.001      |

Based on the results, we can conclude that using a more realistic environment does not improve the model's performance; in fact, it may even worsen it. Cause the performence of Experiment 23 is worse than Experiment 21.

## Experiment 24
In Experiment 24, I retrained the model from Experiment 21 using data from the time period 2021 to 2022 and tested it on 2023 without a stop-loss strategy. This decision was based on prior experiments, which showed that using a stop-loss strategy does not improve performance. The results indicate that in 2023, the model achieved an ROI of 174%, an IRR of 174%, and a Sharpe ratio of 0.88. The training environment was the same as in Experiment 21.

- initial balance : 500000
- train env : StockEnvNoLimit-v1
- test env : StockEnvMargin-v1
- test date : 2023~2023


| experiment             | ROI   |IRR      | Sharp ratio |
| ----                   | ----  | ----    | ----        |
|24 (no stoploss)        | 174%  | 174%    | 0.88        |

## Experiment 25
In Experiment 25, the setup is almost the same as in Experiment 18, but the training environment is different. It uses StockEnvOnlyOneShare-v2, and the training period is from 2021 to 2022. Additionally, the contract price in this environment changes dynamically. The results show that the model achieved an ROI of 113%, an IRR of 113%, and a Sharpe ratio of 1.33.


- initial balance : 100000
- train env : StockEnvOnlyOneShare-v2
- test env : StockEnvOnlyOneShareTesting-v2
- test date : 2023~2023
  
| experiment             | ROI   |IRR      | Sharp ratio |
| ----                   | ----  | ----    | ----        |
|25 (no stoploss)        | 113%  | 113%    | 1.33        |

- initial balance : 100000
- train env : StockEnvOnlyOneShare-v1
- test env : StockEnvOnlyOneShareTesting-v1
- test date : 2023~2023
  
| experiment             | ROI   |IRR      | Sharp ratio |
| ----                   | ----  | ----    | ----        |
|18 (no stoploss)        | 137%  | 137%    | 1.67        |


## Experiment 26
In this experiment, the setup is the same as in Experiment 18, but with a different environment. This environment allows the agent to trade in both long and short positions, and the contract price changes dynamically. The results show that its performance is worse than in Experiment 18. This suggests that allowing the agent to trade in both positions does not improve its performance. I think I need to explore other methods to address the short position issue.


- initial balance : 100000
- train env : StockEnvOnlyOneShare-v3
- test env : StockEnvOnlyOneShareTesting-v3
- test date : 2021~2023
  
| experiment            | ROI   |IRR  | Sharp ratio |
| ----                  | ----  | ----| ---- |
|26 (no stoploss)       | 109%  | 27% | 0.21 |
|26 (stoploss 25 point) | 178%  | 40% | 0.38 |
|26 (stoploss 50 point) | 227%  | 48% | 0.48 |
|26 (stoploss 75 point) | 101%  | 26% | 0.24 |
|26 (stoploss 100 point)| 262%  | 53% | 0.45 |
|26 (stoploss 125 point)| 133%  | 32% | 0.26 |
|26 (stoploss 150 point)| 55%   | 15% | 0.10 |
|26 (stoploss 175 point)| 179%  | 40% | 0.18 |
|26 (stoploss 200 point)| 100%  | 26% | 0.22 |


- initial balance : 100000
- train env : StockEnvOnlyOneShare-v1
- test env : StockEnvOnlyOneShareTesting-v1
- test date : 2021~2023
  
| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|18 (no stoploss)       | 100%  | 26% | 0.19 |
|18 (stoploss 25 point) | 291%  | 57% | 0.64 |
|18 (stoploss 50 point) | 327%  | 62% | 0.73 |
|18 (stoploss 75 point) | 204%  | 44% | 0.43 |
|18 (stoploss 100 point)| 235%  | 49% | 0.42 |
|18 (stoploss 125 point)| 126%  | 31% | 0.26 |
|18 (stoploss 150 point)| 80%   | 21% | 0.16 |
|18 (stoploss 175 point)| 179%  | 40% | 0.35 |
|18 (stoploss 200 point)| 100%  | 26% | 0.22 |


## Experiment 27

In Experiment 27, I used StockEnvOnlyOneShare-v3 to train the agent. In this environment, the agent can take both long and short positions. The training parameters were the same as in Experiment 18, and the training period was from 2021 to 2022. Since 2022 experienced some downward trends, I wanted to check if the agent could learn how to trade in short positions. However, the result, stored in a CSV file, shows that the agent was not able to learn how to trade in short positions.

- initial balance : 100000
- train env : StockEnvOnlyOneShare-v3
- test env : StockEnvOnlyOneShareTesting-v3
- test date : 2023~2023
  
| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|27 (no stoploss)       | 187%  | 187% | 2.14 |

## Experiment 28

In Experiment 28, I used StockEnvOnlyOneShare-v4 to train the agent. In this environment, the agent can only trade one unit and can only take short positions. The training parameters were the same as in Experiment 18, and the training period was from 2018 to 2020. I wanted to see if the agent could learn how to trade in short positions. However, the results show that its performance was poor.

- initial balance : 100000
- train env : StockEnvOnlyOneShare-v4
- test env : StockEnvOnlyOneShareTesting-v4
- test date : 2021~2023
  
| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|28 (no stoploss)       | -51%  | -21% | -0.37 |


## Observations and Conclusions of Experiment 19 to 28
In Experiments 19 to 28, I tried to expand the units that the agent can trade and trained a short trader, but the results didn't look good. `Therefore, the experiments from 1 to 28 are all considered failed and will not be used`.

## Experiment 29
Since the agent in Experiment 28 couldn't learn how to trade in short positions, I think this might be due to an imbalanced dataset. In the dataset I used before this experiment, the number of rising months was greater than the number of declining months, making it difficult for the agent to learn short trading strategies. Based on this assumption, I created synthetic data by reversing the price trend of every month, turning original rising trend months into declining trend months, thus balancing the dataset.

In this experiment, `I first trained an agent that can only trade one unit and only take long positions.` I aim to compare its performance with Experiment 18 to observe the difference between using an imbalanced dataset and a balanced dataset. I used StockEnvOnlyOneShare-v5 to train the agent.

The results show that its performance is worse than in Experiment 18, and this might be an expected outcome.

- initial balance : 100000
- train env : StockEnvOnlyOneShare-v5
- test env : StockEnvOnlyOneShareTesting-v5
- test date : 2021~2023
  
| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|29 (no stoploss)       | 35%  | 10% | 0.07 |

- initial balance : 100000
- train env : StockEnvOnlyOneShare-v1
- test env : StockEnvOnlyOneShareTesting-v1
- test date : 2021~2023
  
| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|18 (no stoploss)       | 100%  | 26% | 0.19 |

## Experiment 30
In Experiment 30, I used StockEnvOnlyOneShare-v6 to train the agent. This environment is similar to the one used in Experiment 29, with the only difference being that `this environment allows the agent to only take short positions and can only trade 1 unit.` I wanted to check if the agent could perform well when facing a decreasing trend.

Although the results didn't perform well, `I found some interesting observations. For example, when Experiment 30 trades during a decreasing trend month, it can earn money, while Experiment 29 cannot.` However, `when they trade during rising trend months, Experiment 30 cannot earn money, but Experiment 29 can.` I also discovered that `if the trend is unstable in a month (e.g., rising in the first half and then decreasing), both agents will lose money.` Therefore, I believe that if I can find a way to combine the advantages of both agents, it could perform well in any situation.


- initial balance : 100000
- train env : StockEnvOnlyOneShare-v6
- test env : StockEnvOnlyOneShareTesting-v6
- test date : 2021~2023
  
| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|30 (no stoploss)       | -82%  | -44% | -0.61 |

## Experiment 31
env : StockEnvOnlyOneShare-v7
In Experiment 31, I'm going to train an agent that can trade with both positions using a balanced dataset. The result doesn't look good, so this experiment is considered failed.

## Experiment 32
In this experiment I futher train an agent that trained in Experiment 29, I let agent can trade more than one unit and restirct it can only do long position. In this experiment the dataset used is synthetic data.

- initial balance : 500000
- train env : StockEnvNoLimit-v3
- test env : StockEnvMarginSyn
- test date : 2021~2023
- dataset synthetic_data

| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|32 (no stoploss)       |277%  | 55% | 0.24 |

> **Info**  
> The model train by this model is going to be chose as a long trader.

## Experiment 33 
In this experiment I futher train an agent that trained in Experiment 30, I let agent can trade more than one unit and restirct it can only do short position. In this experiment the dataset used is synthetic data.

- initial balance : 500000
- train env : StockEnvNoLimit-v4
- test env : StockEnvMarginSynShort
- test date : 2021~2023
- dataset synthetic_data
| experiment | ROI |IRR| Sharp ratio |
| ---- | ---- | ---- | ---- |
|33 (no stoploss)       | -87%  | -50% | -0.38 |

Although the result of this experiment doesn't look good, the model actually works—it can make a profit when facing a downtrend.

> **Info** 
> The model train by this model is going to be chose as a short trader.

## Observations and Conclusions of Experiment 29 to 33
In Experiments 29 to 33, I'm trying to use a balanced dataset to train two agents—one focusing on long trends and the other on short trends. The results suggest that this approach seems to work, as the long agent performs well in long trends, and the short agent performs well in short trends. The next step is to train a trader selector, whose task is to pick the appropriate agent at the right time.


## CombineTest
According to the results of Experiments 29 to 33, I will first analyze the performance of the trader selector. In CombineTest, I am testing how the accuracy of the trader selector influences overall performance. The results show that once the accuracy reaches 60%, it can achieve great performance.


## Experiment 34
因發現原preprocess.py會導致一些資料錯物，因此修正後重新訓練experiment 30。舊資料集改為synthetic_data_before_exe34
並且使用新的資料集，新資料集中trend改為根據每日漲幅來設定，trend len也設定為76，即為一天的k棒數
- dataset synthetic_data_dailtMethod

## Experiment 35
也是一樣使用新資料集重新訓練experiment 31
- dataset synthetic_data_dailtMethod

## Experiment 36
也是一樣使用新資料集重新訓練experiment 29
- dataset synthetic_data_dailtMethod

## Experiment 37
跟experiment 34一樣 只是用別的server跑，因為4090塞滿了，這個用.44跑的
- dataset synthetic_data_dailtMethod

## Experiment 38
重新訓練experiment 36使其能交易更大數量的期貨，env : StockEnvNoLimit-v3_2
- dataset synthetic_data_dailtMethod

## Experiment 39
重新訓練experiment 37使其能交易更大數量的期貨，env : StockEnvNoLimit-v4_2
- dataset synthetic_data_dailtMethod

# TODO: 
- [x] Add sharp ratio into the reward function to reduce risk. (Adding the Sharpe ratio to the reward function does not significantly affect the results, and there may be better methods.)
- [x] Try different initiall balances. (I didn't observe any significant effects.)
- [x] Try different training set, e.g. 2015-2020. The MTX began offering night trading sessions on May 15, 2017. (The futures night trading session was only opened in 2018, so it is not possible to conduct experiments before that.)
- [x] Allow the agent to take both long and short positions. (The agent does not perform well in short positions.)
- [x] The agent can observe over a longer time step. For example, each time step is 15 minutes, and I will allow the agent to observe more time steps at a time.
- [ ] Attempt to use LSTM as the backbone of the model.
- [ ] Use the differential Sharpe ratio as the reward function.
- [ ] Figure out a new method to address the short position issue.
- [ ] Change the policy inside the model.
- [ ] 將時間序列反轉製作人工資料(將上漲月份反轉變成下跌月份)
股市資料來源:https://enochyu.pixnet.net/blog/post/25762671?utm_source=PIXNET&utm_medium=pcard_article&utm_content=5915057172f6ad6e8f

擁有者email:enochyu.trader@gmail.com

股市60分k資料:https://drive.google.com/drive/u/2/folders/12OOHWL0XtcznvfG2Dac8b856-U9pGjTC
